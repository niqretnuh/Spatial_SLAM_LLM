Below is a clean, engineering-ready requirements list for your system. It separates functional requirements, non-functional requirements, data formats, API expectations, and UI/UX requirements ‚Äî so engineers can immediately begin implementation.

‚∏ª

üìò Product Requirements Document (PRD) ‚Äî ‚ÄúThird Eye‚Äù Visual Intelligence Platform

1. Core Concept

A web-based tool that allows users to upload or record a video, runs a computer vision pipeline to extract object detections + depth estimates, then uses an LLM/VLM to generate domain-specific insights.
Users replay the annotated video with visual overlays and interact via chat or voice.

‚∏ª

2. Functional Requirements

2.1 Video Input

Upload
	‚Ä¢	Support .mp4, .mov, .avi, .webm.
	‚Ä¢	Enable drag-and-drop and file picker.
	‚Ä¢	Client uploads file using multipart/form-data to backend endpoint /process-video.

Record
	‚Ä¢	Access webcam using navigator.mediaDevices.getUserMedia().
	‚Ä¢	Implement in-browser recording using MediaRecorder API or react-webcam.
	‚Ä¢	Capture video as .webm blob.
	‚Ä¢	Upload blob to the same /process-video endpoint.

Preview
	‚Ä¢	Show video thumbnail or first frame once selected.
	‚Ä¢	Allow user to click ‚ÄúProcess Video.‚Äù

‚∏ª

2.2 CV Pipeline Processing (Backend)
	‚Ä¢	Accept uploaded video or recorded blob.
	‚Ä¢	Run the existing CV pipeline on server or GPU node.
	‚Ä¢	Extract:
	‚Ä¢	per-frame timestamps
	‚Ä¢	bounding boxes (pixel coords)
	‚Ä¢	object labels
	‚Ä¢	object depth estimates (in meters or normalized units)
	‚Ä¢	optional segmentation masks
	‚Ä¢	Store results in a structured, frame-indexed format (JSON).

Output JSON Format

Engineers must return structured results as:

{
  "video_id": "123abc",
  "fps": 30,
  "frames": [
    {
      "time": 0.033,
      "objects": [
        {
          "id": "obj_1",
          "label": "ladder",
          "bbox": [x1, y1, x2, y2],
          "depth": 2.8
        }
      ]
    }
  ]
}

	‚Ä¢	Store processed results in DB or filesystem associated with video_id.

‚∏ª

2.3 Domain Selection
	‚Ä¢	User selects analysis mode:
	‚Ä¢	Accessibility (low vision safety)
	‚Ä¢	Construction OSHA safety
	‚Ä¢	Fall-prevention
	‚Ä¢	General insight mode
	‚Ä¢	Custom: user text input describing domain
	‚Ä¢	Selected mode is included in LLM prompt.

‚∏ª

2.4 Insight Generation (LLM/VLM Integration)
	‚Ä¢	Backend takes CV pipeline results + user domain input.
	‚Ä¢	Sends structured prompt to LLM (OpenAI API or internal VLM).
	‚Ä¢	LLM returns frame-specific or object-specific insights, e.g.:

{
  "frame_131": [
    {
      "object_id": "obj_1",
      "insight": "This ladder is not secured at its base. OSHA 1926.1053(b)(6)."
    }
  ]
}

	‚Ä¢	Cache or store LLM results linked to video_id.

‚∏ª

2.5 Annotated Video Playback (Frontend)

Video Player
	‚Ä¢	Use native <video> tag or react-player.
	‚Ä¢	Support:
	‚Ä¢	play/pause
	‚Ä¢	timeline scrub
	‚Ä¢	frame-accurate time matching

Overlay Layer (Canvas)
	‚Ä¢	Use react-konva for drawing overlays.
	‚Ä¢	For each frame:
	‚Ä¢	Draw bounding boxes from CV JSON.
	‚Ä¢	Display label & depth next to box.
	‚Ä¢	Display LLM insights anchored to objects.
	‚Ä¢	Allow click on a box to show extended insight in sidebar.

Synchronization
	‚Ä¢	Every requestAnimationFrame, align the current video time with nearest CV frame.
	‚Ä¢	Update overlay layers accordingly.

‚∏ª

2.6 Chat Interface
	‚Ä¢	User can ask questions via text or speech.
	‚Ä¢	Questions + selected video_id sent to backend /query-insights.
	‚Ä¢	Backend passes user question + CV results + LLM insights ‚Üí LLM.
	‚Ä¢	LLM returns contextual answer about the video.

Examples:
	‚Ä¢	‚ÄúWhat hazards do you see here?‚Äù
	‚Ä¢	‚ÄúExplain anything dangerous within 3 meters.‚Äù
	‚Ä¢	‚ÄúSummarize accessibility issues.‚Äù

‚∏ª

2.7 Voice Interface
	‚Ä¢	Implement microphone button using Web Speech API or Whisper.
	‚Ä¢	Convert voice ‚Üí text.
	‚Ä¢	Send text to LLM.
	‚Ä¢	Display results.

‚∏ª

3. Non-Functional Requirements

Performance
	‚Ä¢	Video upload max size: 400MB.
	‚Ä¢	Pipeline processing must handle 30‚Äì60 fps.
	‚Ä¢	Overlay rendering must maintain 30fps on mid-tier laptops.

Scalability
	‚Ä¢	CV pipeline should be containerized (Docker) for GPU option.
	‚Ä¢	Support parallel processing via queue or job runner (optional for MVP).

Security
	‚Ä¢	Sanitize user-uploaded files.
	‚Ä¢	No long-term storage of videos unless user opts in.
	‚Ä¢	HTTPS required.

Cross-Platform
	‚Ä¢	Fully functional on Chrome, Safari, Firefox.
	‚Ä¢	Mobile recording optional but not required for MVP.

‚∏ª

4. Technical Stack Requirements

Frontend
	‚Ä¢	React
	‚Ä¢	react-konva (canvas overlays)
	‚Ä¢	TailwindCSS (UI)
	‚Ä¢	react-webcam or MediaRecorder API (video record)
	‚Ä¢	React Query or Axios (API communication)

Backend
	‚Ä¢	FastAPI / Flask / Node.js (engineers choose)
	‚Ä¢	Video upload & storage (local FS, S3, or Supabase)
	‚Ä¢	CV pipeline runner (Python)
	‚Ä¢	OpenAI or custom LLM API
	‚Ä¢	Redis or DB for caching results (optional)

Data Storage
	‚Ä¢	/uploads/{video_id}.mp4
	‚Ä¢	/results/{video_id}.json (CV annotations)
	‚Ä¢	/insights/{video_id}.json (LLM insights)

‚∏ª

5. End-to-End Processing Flow
	1.	User uploads / records video
	2.	Backend stores video
	3.	CV pipeline runs ‚Üí generates annotations JSON
	4.	User selects domain or describes their goal
	5.	LLM generates contextual insights
	6.	Frontend displays annotated video with overlays
	7.	User interacts via chat/voice for deeper insight
	8.	Optional: export annotated video or insights

‚∏ª

6. Success Criteria (MVP)
	‚Ä¢	Users can upload or record a video.
	‚Ä¢	CV pipeline runs and outputs object labels + depth.
	‚Ä¢	LLM generates contextual insights for the domain selected.
	‚Ä¢	Annotated video playback with overlays is smooth and synchronized.
	‚Ä¢	User can ask natural-language questions and get useful interpretations.

‚∏ª

If you want, I can also produce:
	‚Ä¢	a backend API spec
	‚Ä¢	a wireframe UI mockup
	‚Ä¢	a task-level engineering roadmap
	‚Ä¢	a one-page pitch for hackathon judges

Just tell me what you want next.